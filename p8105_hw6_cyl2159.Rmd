---
title: "Linear Models"
author: "Chhiring Lama"
date: "2024-11-27"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(latex2exp)
library("PerformanceAnalytics")
library(modelr)
library(mgcv)
library(SemiPar)
library(ggpubr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "85%", 
	fig.align = "center"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(2)
```

## Problem 1 
```{r, warning = FALSE, message = FALSE}
weather_df <-  
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Running simple linear regression with `tmax` as the response and `tmin` as the predictor in 5000 bootstrap samples:
```{r}
bootstraps <- weather_df |> 
  modelr::bootstrap(5000) |> 
  mutate(strap = map(strap, as_tibble), 
         linear_model = map(strap, \(df) lm(tmax ~ tmin, data = df)), 
    results = map(linear_model, broom::tidy), 
    summary = map(linear_model, broom::glance))

bootstrap_r_squared <- bootstraps |> 
  unnest(summary) |> 
  select(.id, `r.squared`) 

bootstrap_log_estimate <- bootstraps |> 
  unnest(results) |> 
  select(.id, estimate) |> 
  group_by(`.id`) |> 
  summarize(log_estimate = prod(estimate) |> log()) 

merged_results <- bootstrap_r_squared |> 
  inner_join(bootstrap_log_estimate, by = ".id")
```

Plot the distribution of the $r^2$ and $log(\hat{\beta_0} *\hat{\beta_1})$ estimates. 
```{r, fig.width = 5, fig.height = 4, out.width = "95%", dpi = 600}
merged_results |> 
  pivot_longer(
    cols = 2:3, 
    names_to = "type",
    values_to = "estimate"
  ) |> 
  mutate(
    labels = case_when(type == "log_estimate" ~ "log(hat(beta[0])*hat(beta[1]))", 
                      type == "r.squared" ~ "r^2")) |> 
  ggplot(aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~labels, scales = "free", 
             ncol = 1, labeller = label_parsed) +
  theme(legend.position = "none") +
  labs(x = "Estimate", 
       y = "Density", title = "Distribution of the Estimates")
```

The estimates of $r^2$ and $log(\hat{\beta_0}*\hat{\beta_1})$ (as shown above) are normally distributed. 

```{r}
r_squared_ci <- bootstrap_r_squared |> 
  summarise(r_squared_ci_ll = quantile(`r.squared`, 0.025) |> round(digits = 3), 
            r_squared_ci_ul = quantile(`r.squared`, 0.975) |> round(digits = 3))

log_estimate_ci <- bootstrap_log_estimate |> 
  summarise(log_estimate_ci_ll = quantile(log_estimate, 0.025) |> round(digits = 3), 
            log_estimate_ci_ul = quantile(log_estimate, 0.975) |> round(digits = 3))

r_squared_ci |> 
  knitr::kable()

log_estimate_ci |> 
  knitr::kable()
```
95% confidence interval for $r^2$ and $log(\hat{\beta_0}*\hat{\beta_1})$ are (`r pull(r_squared_ci, r_squared_ci_ll)`, `r pull(r_squared_ci, r_squared_ci_ul)`) and (`r pull(log_estimate_ci, log_estimate_ci_ll)`, `r pull(log_estimate_ci, log_estimate_ci_ul)`) respectively. We are 95% sure that the model explains between `r pull(r_squared_ci, r_squared_ci_ll)` and `r pull(r_squared_ci, r_squared_ci_ul)` of variance in the maximum temperature. 

## Problem 2

Import and clean homicide data from the Washington Post:
```{r, warning = FALSE, message = FALSE}
homicide_data <- read_csv("data/homicide-data.csv") |> 
  mutate(state = toupper(state), 
         city_state = str_c(city, state, sep = ", "), 
         result = case_when(disposition %in% c("Closed without arrest", 
                                                "Open/No arrest") ~ "unresolved", 
                            disposition == "Closed by arrest" ~ "resolved"), 
         result = as_factor(result), 
         victim_age = as.numeric(victim_age), 
         result = as.numeric(result == "resolved"),
    victim_race = fct_relevel(victim_race, "White")) |> 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", 
                            "Kansas City, MO","Tulsa, AL"), 
         victim_race %in% c("White", "Black"))
```

Run logistic Regression with `result` (resolved versus unresolved) as the outcome and victim age, sex and race as predictors for Baltimore, MD. 
```{r,warning = FALSE, message = FALSE}
baltimore_df <- homicide_data |> 
  filter(city_state == "Baltimore, MD") |> 
  select(result, victim_age, victim_race, victim_sex)

logistic_mod <- glm(result ~ victim_age + victim_sex + victim_race, 
                    data = baltimore_df, family = binomial())
saveRDS(logistic_mod, "logistic_mod_baltimore_result.rds")

logistic_mod <- logistic_mod |> 
  broom::tidy() |> 
  mutate(adj_OR = exp(estimate), 
         ci_ll = round(exp(estimate - 1.96 * std.error), 3), 
         ci_ul = round(exp(estimate + 1.96 * std.error), 3)) |>
  select(term, adj_OR, ci_ll, ci_ul, p.value) 

res_male <- logistic_mod |> 
  filter(term == "victim_sexMale") 

or_male <- pull(res_male, `adj_OR`) |> 
  round(digits = 3)


res_male |> 
  mutate_if(is.numeric, signif, 3) |> 
  knitr::kable(digits = 11)

```
Upon running the logistic regression, at 5% significance level and after keeping all other variables fixed, we have significant evidence that male victims have lower probability (`r 1 - or_male` less likely) of having homicides solved compared to female victims. We are 95% confident that the true adjusted odds ratio is between `r pull(res_male, ci_ll) |> round(digits = 3)` and `r pull(res_male, ci_ul) |> round(digits = 3)`. 

Run the model for each city in the dataset
```{r}
logistic_allcities_df <- homicide_data |> 
  filter(victim_sex %in% c("Female", "Male")) |> 
  nest(data = -city_state) |> 
  mutate(
    model = map(data, \(x) glm(result ~ victim_age + victim_sex + victim_race, 
                    data = x, family = binomial())), 
    results = map(model, broom::tidy)
  ) |> 
  select(city_state, results) |> 
  unnest(results) |> 
  filter(term == "victim_sexMale") |> 
  mutate(adj_OR = exp(estimate), 
         ci_ll = round(exp(estimate - 1.96 * std.error), 3), 
         ci_ul = round(exp(estimate + 1.96 * std.error), 3)) |>
  select(city_state, term, adj_OR, ci_ll, ci_ul, p.value) 
```

Plot ORs and CIs for each city
```{r, fig.width = 6, fig.height = 7, out.width = "95%", dpi=600}
logistic_allcities_df |> 
  mutate(city_state = fct_reorder(city_state, adj_OR)) |>
  ggplot(aes(x = city_state, y = adj_OR)) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey") +
  geom_errorbar(aes(ymin = ci_ll, ymax = ci_ul),color = "darkred", width = 0.5)+
  geom_point(size = 2, shape = 21, fill = "white") +
  theme(plot.title = element_text(size = 10, face = "bold"), 
        axis.title = element_text(size = 9, face = "bold")) +
  labs(y = "Adjusted Odds Ratio", x = "City",
       title = "Adj. OR (w/ 95% CI) for Solved Cases in Male v/s Female", color = "") +
  coord_flip()
```
There are four cities where estimated adjusted odds ratio is greater than one (Albuquerque, Stockton, Fresno and Nashville) which means that male victims in these cities are more likely to have their case resolved compared to female victims. In Richmond and Atlanta, male and females have almost equal chance of solved case. In all the other cities, male victims have less odds of a resolved homicide case than female victims. 

## Problem 3
Load and clean the child birthweight dataset.
```{r, warning = FALSE, message = FALSE}
child_df <- read_csv("data/birthweight.csv") |> 
  mutate(
    babysex = case_when(babysex == 1 ~ "Male", 
                        babysex == 2 ~ "Female"), 
    babysex = fct_relevel(babysex, "Male"), 
    frace = case_when(frace == 1 ~ "White", 
                      frace == 2 ~ "Black", 
                      frace == 3 ~ "Asian", 
                      frace == 4 ~ "Puerto Rican", 
                      frace == 8 ~ "Other", 
                      frace == 9 ~ "Unknown"),
    frace = fct_relevel(frace, "White"), 
    malform = case_when(malform == 0 ~ "absent", 
                        malform == 1 ~ "present"), 
    malform = fct_relevel(malform, "absent"), 
    mrace = case_when(mrace == 1 ~ "White", 
                      mrace == 2 ~ "Black", 
                      mrace == 3 ~ "Asian", 
                      mrace == 4 ~ "Puerto Rican", 
                      mrace == 8 ~ "Other"),
    mrace = fct_relevel(mrace, "White")
    )

missing_value_check <- unique(is.na(child_df))
```
The child birthweight dataset consists of 4 categorical variables (`babysex`, `frace`, `malform` and `mrace`) representing sex of the baby, father's race, presence of any malformation that could affect baby's weight and mother's race. There are 16 numerical variables. 

I am interested to check if birthweight can be predicted by using mother's delivery weight while accounting for all other observations related to her. So, first I checked the correlation between related continuous variables. 
```{r, fig.width = 6, fig.height = 7, out.width = "99%", dpi=600}
my_data <- child_df[, c("delwt","menarche", "ppbmi", "wtgain", "mheight", "ppwt", 
                        "momage", "gaweeks", "pnumsga", "parity", "smoken")]
chart.Correlation(my_data, histogram=TRUE, pch=19)
```

Mother's pre-pregancy BMI - which is based on their height/weight at that point - correlates with weight at delivery too, so I used `delwt` along with `ppbmi` along with interaction terms to test their combined effects. The value for `ppbmi` is derived from `mheight` and `ppwt`, so using latter these variable will be redundant. Number of prior small for gestational age babies all have value of 0 and number of live births prior to this pregnancy are mostly 0, so they are not an informative variable. Therefore, I first start by proposing a multiple linear regression for birthweight as `delwt`, `menarche`, `wtgain`, `ppbmi`, `momage`, `gaweeks`, `smoken` and additional categorical variable: mother's race. 

```{r}
child_df <- child_df |> 
  mutate(id = row_number())

train_df <- sample_frac(
  child_df, 
  size = .8
)

test_df <- anti_join(child_df, train_df, by = "id")
```

Visualize the relation between birthweight and mother's weight during delivery
```{r}
train_df |> 
  ggplot(aes(x = delwt, y = bwt), size = 0.2) + 
  geom_point()+
  geom_point(data = test_df, color = "skyblue") +
  scale_color_discrete(labels = c("Training", "Test")) +
  geom_smooth(method = lm, se = FALSE)+ 
  stat_cor(label.x = 200, label.y = 1000)
```

Fit the linear multiple regression model with the above mentioned predictors
```{r}
linear_mod = lm(bwt ~ delwt*ppbmi + menarche + momage*gaweeks +smoken +mrace, data = train_df)
linear_mod |> 
  broom::tidy() 

linear_mod |> 
  broom::glance() 
```

As we can see above, at 5% significance level, mother's menarche. So, I removed the predictor from the model
```{r}
linear_modv2 = lm(bwt ~ delwt*ppbmi + momage*gaweeks +smoken +mrace, data = train_df)
linear_modv2 |> 
  broom::tidy()
linear_modv2 |> 
  broom::glance() 
```

Look at the fits in the training data:
```{r, fig.width = 5, fig.height = 3, out.width = "80%", dpi=600}
train_df |> 
  add_predictions(linear_modv2) |> 
  add_residuals(linear_modv2) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_hline(yintercept = 0, color = "red", alpha = 0.6, linetype = "dotted") +
  geom_point(size = 0.4, color = "black", shape = 21)+
  geom_smooth(method = lm, se = FALSE)+ 
  labs(title = "Residual Versus Fitted Values")
```
The "spread" of the points in the plot are remain similar across different predicted values, and their residuals do not follow a specific "trend". So, we can assume linearity and homoscedasticity in the model above. 

Adding the two other models and cross-validation of all three models: 
* Model1: Linear model above with mothers' delivery weight, age, race, gestational period and number of cigarettes smokes during pregnancy. 
* Model2: One using length at birth and gestational age as predictors (main effects only)
* Model3: One using head circumference, length, sex, and all interactions (including the three-way interaction) between these.

```{r}
cv_df <- crossv_mc(child_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
  

cv_res_df <- cv_df |> 
  mutate(
    mod_w_mom = map(train, \(x) lm(bwt ~ delwt + momage*gaweeks + smoken + mrace, data = x)), 
    mod_w_baby1 = map(train, \(x) lm(bwt ~ blength + gaweeks, data = x)), 
    mod_w_baby2 = map(train, \(x) lm(bwt ~ bhead*blength*babysex, data = x))
  ) |> 
  mutate(
    rmse_w_mom = map2_dbl(mod_w_mom, test, rmse), 
    rmse_w_baby1 = map2_dbl(mod_w_baby1, test, rmse), 
    rmse_w_baby2 = map2_dbl(mod_w_baby2, test, rmse)
  )
  
```

```{r, fig.width = 5, fig.height = 3, out.width = "80%", dpi=600}
cv_res_df |> 
  select(starts_with("rmse"))  |> 
  pivot_longer(
    everything(),
    names_to = "model_type", 
    values_to = "rmse"
  ) |> 
  ggplot(aes(x = model_type, y = rmse)) +
  geom_violin() +
  scale_x_discrete(labels = c("RMSE: Model 1", "RMSE: Model 2", "RMSE: Model 3"))
  
```

After comparing the tree models, Model 3 (using head circumference, length, sex, and their interactions) has the the least root mean squared error (RMSE). Therefore, it predicts baby's birthweight most accurately, followed by Model 2 and then Model 1. 

